{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "ec3d28a1-327c-4d1e-b69c-b86ab754ddb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "libraries imported ðŸ˜Ž\n"
     ]
    }
   ],
   "source": [
    "import re \n",
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer ## stemming \n",
    "from nltk.stem import WordNetLemmatizer  ## lemmatization\n",
    "import joblib,os \n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import pad_sequences \n",
    "from sklearn.preprocessing import LabelEncoder ,OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "print('libraries imported ðŸ˜Ž')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "349645d0-6f25-460c-b7e2-b12622a69268",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading data from the text files \n",
    "\n",
    "train_data = open(\"./Data/train.txt\").readlines()\n",
    "val_data = open(\"./Data/val.txt\").readlines()\n",
    "test_data = open(\"./Data/test.txt\").readlines()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a986b3ce-8dcd-4c90-ba19-3962d8ae3de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16000\n",
      "2000\n",
      "2000\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(val_data))\n",
    "print(len(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03fa62f9-f16b-4f72-b304-bff76bdf2355",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "complet_data = train_data + test_data + val_data \n",
    "print(len(complet_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2a524209-6a55-4844-afa9-51cd9842344c",
   "metadata": {},
   "outputs": [],
   "source": [
    "x=[]\n",
    "y=[]\n",
    "for temp in complet_data:\n",
    "    complete = temp.split(';')\n",
    "    if len(complete)==2:\n",
    "        x.append(complete[0])\n",
    "        y.append(complete[1].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5ace463b-dac6-48b5-a4d5-fdd63946b80e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['sadness', 'anger', 'love', 'surprise', 'fear', 'joy']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "for item in y:\n",
    "        if item not in labels:\n",
    "            labels.append(item)\n",
    "labels "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0605f5cc-53fa-494a-8548-0d8d6b6b8cc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Text cleaning \n",
    "# 1. lower case convert all message \n",
    "# 2. a-z0-9   other characters \n",
    "# 3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fec0144-333d-48b1-a014-e9ad361a2161",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "89693722-a988-4a59-8fb1-7c6897a64dd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# text cleaning function applying on each and every raw message \n",
    "def text_cleaning(sentences,stemming):\n",
    "    cleaned_data = []\n",
    "    for sentece in sentences: \n",
    "        message = sentece.lower()\n",
    "        message = re.sub('[^a-z0-9 ]',\"\",message)\n",
    "        ls_of_words = nltk.word_tokenize(message)\n",
    "        ls_of_word_without_stopwords = [word for word in ls_of_words if word not in stopwords.words('english')]\n",
    "        stemmed_words = [stemming.stem(word) for word in ls_of_word_without_stopwords ]\n",
    "        message = \" \".join(stemmed_words)\n",
    "        cleaned_data.append(message)\n",
    "    return cleaned_data\n",
    "\n",
    "stem = PorterStemmer()\n",
    "cleaned_data = text_cleaning(x,stemming=stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e5d6e6ff-eaa9-4e10-906c-fdc8deb4b54a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully saved your cleaned data!\n"
     ]
    }
   ],
   "source": [
    "os.makedirs('models',exist_ok=True)\n",
    "joblib.dump(cleaned_data,\"./models/cleaned_data.lb\")\n",
    "print(\"successfully saved your cleaned data!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f5a53e7-53df-4028-a484-2ae58bb2cd9c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "be3e92db-b4b2-4711-9355-59f023353ed3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i can go from feeling so hopeless to so damned hopeful just from being around someone who cares and is awake\n",
      "['go', 'feeling', 'hopeless', 'damned', 'hopeful', 'around', 'someone', 'cares', 'awake']\n",
      "['go', 'feel', 'hopeless', 'damn', 'hope', 'around', 'someon', 'care', 'awak']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'go feel hopeless damn hope around someon care awak'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## trying  on a sinlgle message \n",
    "ls_of_stopwords = stopwords.words('english')\n",
    "print(x[1])\n",
    "single_message = x[1].lower()\n",
    "single_message = re.sub('[^a-z0-9 ]',\"\",single_message)\n",
    "ls_of_words = nltk.word_tokenize(single_message)  # single_message.split()\n",
    "ls_of_word_without_stopwords = []\n",
    "for word in ls_of_words: \n",
    "    if word not in ls_of_stopwords:\n",
    "        ls_of_word_without_stopwords.append(word)\n",
    "print(ls_of_word_without_stopwords)\n",
    "stem = PorterStemmer()\n",
    "stemmed_words = []\n",
    "for word in ls_of_word_without_stopwords: \n",
    "    stemmed_words.append(stem.stem(word))\n",
    "print(stemmed_words)\n",
    "\" \".join(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ab042279-feb3-4d6e-8735-60262a2ad641",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "88fc2d12-b41e-4d98-9089-606ae0d4db91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully saved your tokenizer at this location : './models/tokenizer.lb'\n"
     ]
    }
   ],
   "source": [
    "## Tokenaization \n",
    "tokenizer = Tokenizer(oov_token=\"<nothing>\")\n",
    "tokenizer.fit_on_texts(cleaned_data)\n",
    "joblib.dump(tokenizer,'./models/tokenizer.lb')\n",
    "print(\"successfully saved your tokenizer at this location : './models/tokenizer.lb'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "af6c3c72-61a2-4258-9a58-0cc590d2dda3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78ef78b5-93c2-47b0-91b1-f3cc41855ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer.word_counts  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "025611d3-8a94-401a-a5dd-44d6a09999a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_data = tokenizer.texts_to_sequences(cleaned_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "5011cbdd-5574-450f-92f4-682f280917e9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7151c8f6-4f10-42c4-b6c8-e24c58290d85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'didnt feel humili'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b03fc307-c490-4a79-ba01-bbc56b23c312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[61, 2, 522]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "9946d07c-9490-4ea5-8bb6-35363e6308a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer.word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d8273e59-a5d1-444e-8c72-8cf1a7634d37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your maximum length is :  35\n"
     ]
    }
   ],
   "source": [
    "len_of_message = []\n",
    "for i in range(len(tokenized_data)):\n",
    "    len_of_message.append(len(tokenized_data[i]))\n",
    "print(\"Your maximum length is : \",max(len_of_message))\n",
    "# maximum length  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "061b653d-1480-4a4b-9257-2ff0749dc4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your maximum length is :  35\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Your maximum length is : \",max(list(map(len,tokenized_data))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "8117bfbe-fe02-468b-9e15-6e981bbe95f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequences = pad_sequences(tokenized_data,maxlen=35,padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "7aca60da-abfd-4e31-bd46-c7eecc6062f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  61,    2,  522, ...,    0,    0,    0],\n",
       "       [  10,    2,  419, ...,    0,    0,    0],\n",
       "       [   4, 1230,  431, ...,    0,    0,    0],\n",
       "       ...,\n",
       "       [   2,  194,  157, ...,    0,    0,    0],\n",
       "       [ 328,    2,  175, ...,    0,    0,    0],\n",
       "       [   2,    3,  916, ...,    0,    0,    0]])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "b10dfc98-9e4e-4039-9931-1dd3141b6482",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sadness': 0, 'anger': 1, 'love': 2, 'surprise': 3, 'fear': 4, 'joy': 5}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=[]\n",
    "for item in y:\n",
    "        if item not in labels:\n",
    "            labels.append(item)\n",
    "label_dict = {label:i for i , label in enumerate(labels)}\n",
    "label_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "eff6cdb4-bce0-4e24-aa5c-4d45b835ed60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder ,OneHotEncoder\n",
    "# automation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "80814ada-fe8b-4016-86f3-ef6a2fba3c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_encoder = LabelEncoder()\n",
    "Y = label_encoder.fit_transform(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "6c2f31de-645f-4686-af5f-1d617bd89c74",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "a5c0788f-5c6c-47b4-b577-d09264de3d75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'fear', 'joy', 'love', 'sadness', 'surprise'], dtype='<U8')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_encoder.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "447bbcab-c380-4da1-a3c0-921d0a6be1d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./models/label_encoder.lb']"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joblib.dump(label_encoder,'./models/label_encoder.lb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "5ab001c7-8187-4c1f-ac0d-fde170e34ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 4, 0, ..., 2, 2, 2], dtype=int64)"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences    # <====  X DATA  cleaned data \n",
    "Y            # <====  Y DATA label \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "c86d0fb1-36af-4a93-b7a7-42fe7cdf6f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "## train test splitting \n",
    "from sklearn.model_selection import train_test_split\n",
    "x_train,x_test,y_train,y_test = train_test_split(sequences,Y,test_size=0.15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c700cbf-303d-486d-b4bc-fa80ea2b755a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model training\n",
    "## Model define  \n",
    "# ANN , CNN , RNN  \n",
    "### logistic regression ,\n",
    "### DTC , RDC , \n",
    "## machine learning algorithm   | ANN   | RNN \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db387589-4eaa-403a-95ee-1f706afee499",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fb2a300-0f83-4e82-9b7b-c523cc438c2c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e733804b-b5bb-4d8b-a25d-17fdc749177f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65faa4d3-5115-481c-8bb5-02a3acf3140c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
